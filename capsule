本文主要参考了搜狐上的一篇文章，强烈推荐大家看一看http://www.sohu.com/a/218609089_500659
   1、Capsule 是什么？
   传统的神经网络的神经元就只是一个标量，它能代表的信息很少，比如某个神经元表示了眼睛这个特征，那么与其相连的上层的神经元是否包含了这个特征就通过权重的
大小来表征，当然，这也仅仅只能表征是否含有羽毛这个特征，不能具体的给出羽毛的详细信息如：单眼皮还是双眼皮，瞳孔的颜色之类。而capsule神经元则是一个向量，
这个向量本身表示一个特征，向量里面的值则代表了其详细的信息如（0， 2）表示具有单眼皮，蓝色瞳孔的眼睛。这样，capsule在单个特征的表达上就更丰富了。
   2、Capsule 的运算 ?
   假设第l层有4个胶囊（u1,u2,u3,u4），第l+1层有3个胶囊(v1,v2,v3)，其中u1表示羽毛这个特征，v1,v2,v3分别表示鸡，鸭，狗，那么我想知道u1属于上层特
征的哪一个怎么办呢？这就是个简单的分类问题了，内积之后softmax就可以得到它属于鸡鸭狗的概率了,例如：p(v1|u1)=e(u1,v1)/(e(u1,v1)+e(u1,v2)+e(u1,v3))
不过，单靠这个特征还不够，我们还需综合各个特征，于是可以把上述操作对各个 ui 都做一遍，继而得到p(v2|u1),p(v2|u2)....,问题是，现在得到这么多预测结果，
那我究竟要选择哪个呢？而且我又不是真的要做分类，我要的是融合这些特征，构成更高级的特征。于是 Hinton 认为，既然 ui 这个特征得到的概率分布是[p(v1|ui),p(v2|ui),p(v3|ui)]，那么我把这个特征切成三份，分别为 [p(v1|ui)ui,p(v2|ui)ui,p(v3|ui)ui]，然后把这几个特征分别传给 v1,v2,v3最后 v1,v2,v3其实就是各个底层传入的特征的累加再squashing，squashing的出现主要是hinton希望capsule能有一个性质：胶囊的模长能够代表这个特征的概率。
   另外，其实论文中ui和vi之间还需要一个线性变换W，前面已经说了，vj 是作为输入 ui 的某种聚类中心出现的，而从不同角度看输入，得到的聚类结果显然是不一样的。
那么为了实现“多角度看特征”，可以在每个胶囊传入下一个胶囊之前，都要先乘上一个矩阵W做变换。
   从上往下看， Capsule 就是每个底层特征分别做分类，然后将分类结果整合。这时 vj 应该尽量与所有 ui 都比较靠近，靠近的度量是内积。从下往上看的话，可以认
为 vj 实际上就是各个 ui 的某个聚类中心，而 Capsule 的核心思想就是输出是输入的某种聚类结果。
   3、动态路由
   为了得到各个 vj，一开始先让它们全都等于 ui 的均值，然后反复迭代就好。说白了，输出是输入的聚类结果，而聚类通常都需要迭代算法，这个迭代算法就称为“动态路由”。
   for all capsule i in layer l and capsule j in layer (l + 1): bij ← 0.
   for r iterations do
      for all capsule i in layer l: ci ← softmax(bi) .
      for all capsule j in layer (l + 1): sj ←cijˆuj|i
      for all capsule j in layer (l + 1): vj ← squash(sj ) 
      for all capsule i in layer l and capsule j in layer (l + 1): bij ← bij + ˆuj|i.vj
   return vj
  
